# Web-Crawler

This repository involves in most of web cralwer projects I have already written. 

# What is Web Cralwer

Usually, we start to read a page from one website, and get other urls from the webpage. Then we implement dfs or bfs to search for other urls from the urls we get from the previous webpage. Repeatly, we get all pages we want.

The basic operation of a web crawler program is data crawling.

# Involved Areas

DFS, BFS, File System, Multi-threads, anti-cralwer, and etc.
